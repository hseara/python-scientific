{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Numpy and Scipy\n",
    "\n",
    "[Numpy](http://numpy.org) contains core routines for doing fast vector, matrix, and linear algebra-type operations in Python. [Scipy](http://scipy) contains additional routines for optimization, special functions, and so on. Both contain modules written in C and Fortran so that they're as fast as possible. Together, they give Python roughly the same capability that the [Matlab](http://www.mathworks.com/products/matlab/) program offers. (In fact, if you're an experienced Matlab user, there a [guide to Numpy for Matlab users](http://www.scipy.org/NumPy_for_Matlab_Users) just for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making vectors and matrices\n",
    "Fundamental to both Numpy and Scipy is the ability to work with vectors and matrices. You can create vectors from lists using the **array** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass in a second argument to **array** that gives the numeric type. There are a number of types [listed here](http://docs.scipy.org/doc/numpy/user/basics.types.html) that your matrix can be. Some of these are aliased to single character codes. The most common ones are 'd' (double precision floating point number), 'D' (double precision complex number), and 'i' (int32). Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4,5,6],'d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4,5,6],'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,4,5,6],'i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build matrices, you can either use the array command with lists of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.array([[0,1],[1,0]],'d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also form empty (zero) matrices of arbitrary shape (including vectors, which Numpy treats as a one row matrix), using the **zeros** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.zeros((3,3),'d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is a tuple containing the shape of the matrix, and the second is the data type argument, which follows the same conventions as in the array command. Thus, you can make row vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.zeros(3,'d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.zeros((1,3),'d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or column vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.zeros((3,1),'d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also an **identity** command that behaves as you'd expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.identity(4,'d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as a **ones** command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linspace, matrix functions, and plotting\n",
    "The **linspace** command makes a linear array of points from a starting to an ending value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.linspace(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you provide a third argument, it takes that as the number of points in the space. If you don't provide the argument, it gives a length 50 linear space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.linspace(0,1,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**linspace** is an easy way to make coordinates for plotting. Functions in the numpy library (all of which are imported into IPython notebook) can act on an entire vector (or even a matrix) of points at once. Thus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,2*np.pi)\n",
    "np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conjunction with **matplotlib**, this is a nice way to plot things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,np.sin(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "Matrix objects act sensibly when multiplied by scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "0.125*np.identity(3,'d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as when you add two matrices together. (However, the matrices have to be the same shape.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.identity(2,'d') + np.array([[1,1],[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that confuses Matlab users is that the times (*) operator give element-wise multiplication rather than matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.identity(2)*np.ones((2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get matrix multiplication, you need the **dot** command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.dot(np.identity(2),np.ones((2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dot** can also do dot products (duh!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "v = np.array([3,4],'d')\n",
    "np.sqrt(np.dot(v,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as well as matrix-vector products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **determinant**, **inverse**, and **transpose** functions that act as you would suppose. Transpose can be abbreviated with \".T\" at the end of a matrix object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "m = np.array([[1,2],[3,4]])\n",
    "print(\"{0}\\n\".format(m))\n",
    "print(\"{0}\\n\".format(m.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a **diag()** function that takes a list or a vector and puts it along the diagonal of a square matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.diag([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find this useful later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Solvers\n",
    "You can solve systems of linear equations using the **solve** command:\n",
    "\n",
    "x+y+z=6\n",
    "\n",
    "2y+5z=-4\n",
    "\n",
    "2x+5y-z=27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1,1,1],[0,2,5],[2,5,-1]])\n",
    "print(\"{0}\\n\".format(A))\n",
    "\n",
    "b = np.array([6,-4,27])\n",
    "print(\"{0}\\n\".format(b))\n",
    "\n",
    "print(np.linalg.solve(A,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and eigenvalues\n",
    "There are a number of routines to compute eigenvalues and eigenvectors\n",
    "\n",
    "* **eigvals** returns the eigenvalues of a matrix\n",
    "* **eigvalsh** returns the eigenvalues of a Hermitian matrix\n",
    "* **eig** returns the eigenvalues and eigenvectors of a matrix\n",
    "* **eigh** returns the eigenvalues and eigenvectors of a Hermitian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[13,-4],[-4,7]],'d')\n",
    "np.linalg.eigvalsh(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.linalg.eigh(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Finite Differences\n",
    "Now that we have these tools in our toolbox, we can start to do some cool stuff with it. Many of the equations we want to solve in Physics involve differential equations. We want to be able to compute the derivative of functions:\n",
    "\n",
    "$$ y' = \\frac{y(x+h)-y(x)}{h} $$\n",
    "\n",
    "by *discretizing* the function $y(x)$ on an evenly spaced set of points $x_0, x_1, \\dots, x_n$, yielding $y_0, y_1, \\dots, y_n$. Using the discretization, we can approximate the derivative by\n",
    "\n",
    "$$ y_i' \\approx \\frac{y_{i+1}-y_{i-1}}{x_{i+1}-x_{i-1}} $$\n",
    "\n",
    "We can write a derivative function in Python via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def nderiv(y,x):\n",
    "    \"Finite difference derivative of the function f\"\n",
    "    n = len(y)\n",
    "    d = np.zeros(n,'d') # assume double\n",
    "    # Use centered differences for the interior points, one-sided differences for the ends\n",
    "    for i in range(1,n-1):\n",
    "        d[i] = (y[i+1]-y[i-1])/(x[i+1]-x[i-1])\n",
    "    d[0] = (y[1]-y[0])/(x[1]-x[0])\n",
    "    d[n-1] = (y[n-1]-y[n-2])/(x[n-1]-x[n-2])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether this works for our sin example from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,2*np.pi)\n",
    "dsin = nderiv(np.sin(x),x)\n",
    "plt.plot(x,dsin,label='numerical')\n",
    "plt.plot(x,np.cos(x),label='analytical')\n",
    "plt.title(\"Comparison of numerical and analytical derivatives of sin(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Dimensional Harmonic Oscillator using Finite Difference\n",
    "Now that we've convinced ourselves that finite differences aren't a terrible approximation, let's see if we can use this to solve the one-dimensional harmonic oscillator.\n",
    "\n",
    "We want to solve the time-independent Schrodinger equation\n",
    "\n",
    "$$ -\\frac{\\hbar^2}{2m}\\frac{\\partial^2\\psi(x)}{\\partial x^2} + V(x)\\psi(x) = E\\psi(x)$$\n",
    "\n",
    "for $\\psi(x)$ when $V(x)=\\frac{1}{2}m\\omega^2x^2$ is the harmonic oscillator potential. We're going to use the standard trick to transform the differential equation into a matrix equation by multiplying both sides by $\\psi^*(x)$ and integrating over $x$. This yields\n",
    "\n",
    "$$ -\\frac{\\hbar}{2m}\\int\\psi(x)\\frac{\\partial^2}{\\partial x^2}\\psi(x)dx + \\int\\psi(x)V(x)\\psi(x)dx = E$$\n",
    "\n",
    "We will again use the finite difference approximation. The finite difference formula for the second derivative is\n",
    "\n",
    "$$ y'' = \\frac{y_{i+1}-2y_i+y_{i-1}}{x_{i+1}-x_{i-1}} $$\n",
    "\n",
    "We can think of the first term in the Schrodinger equation as the overlap of the wave function $\\psi(x)$ with the second derivative of the wave function $\\frac{\\partial^2}{\\partial x^2}\\psi(x)$. Given the above expression for the second derivative, we can see if we take the overlap of the states $y_1,\\dots,y_n$ with the second derivative, we will only have three points where the overlap is nonzero, at $y_{i-1}$, $y_i$, and $y_{i+1}$. In matrix form, this leads to the tridiagonal Laplacian matrix, which has -2's along the diagonals, and 1's along the diagonals above and below the main diagonal.\n",
    "\n",
    "The second term turns leads to a diagonal matrix with $V(x_i)$ on the diagonal elements. Putting all of these pieces together, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def Laplacian(x):\n",
    "    h = x[1]-x[0] # assume uniformly spaced points\n",
    "    n = len(x)\n",
    "    M = -2*np.identity(n,'d')\n",
    "    for i in range(1,n):\n",
    "        M[i,i-1] = M[i-1,i] = 1\n",
    "    return M/h**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3)\n",
    "m = 1.0\n",
    "ohm = 1.0\n",
    "T = (-0.5/m)*Laplacian(x)\n",
    "V = 0.5*(ohm**2)*(x**2)\n",
    "H =  T + np.diag(V)\n",
    "E,U = np.linalg.eigh(H)\n",
    "h = x[1]-x[0]\n",
    "\n",
    "# Plot the Harmonic potential\n",
    "plt.plot(x,V,color='k')\n",
    "\n",
    "for i in range(4):\n",
    "    # For each of the first few solutions, plot the energy level:\n",
    "    plt.axhline(y=E[i],color='k',ls=\":\")\n",
    "    # as well as the eigenfunction, displaced by the energy level so they don't\n",
    "    # all pile up on each other:\n",
    "    plt.plot(x,-U[:,i]/np.sqrt(h)+E[i])\n",
    "plt.title(\"Eigenfunctions of the Quantum Harmonic Oscillator\")\n",
    "plt.xlabel(\"Displacement (bohr)\")\n",
    "plt.ylabel(\"Energy (hartree)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made a couple of hacks here to get the orbitals the way we want them. First, I inserted a -1 factor before the wave functions, to fix the phase of the lowest state. The phase (sign) of a quantum wave function doesn't hold any information, only the square of the wave function does, so this doesn't really change anything. \n",
    "\n",
    "But the eigenfunctions as we generate them aren't properly normalized. The reason is that finite difference isn't a real basis in the quantum mechanical sense. It's a basis of Dirac Î´ functions at each point; we interpret the space betwen the points as being \"filled\" by the wave function, but the finite difference basis only has the solution being at the points themselves. We can fix this by dividing the eigenfunctions of our finite difference Hamiltonian by the square root of the spacing, and this gives properly normalized functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Functions\n",
    "The solutions to the Harmonic Oscillator are supposed to be Hermite polynomials. The Wikipedia page has the HO states given by\n",
    "\n",
    "$$\\psi_n(x) = \\frac{1}{\\sqrt{2^n n!}}\n",
    "\\left(\\frac{m\\omega}{\\pi\\hbar}\\right)^{1/4}\n",
    "\\exp\\left(-\\frac{m\\omega x^2}{2\\hbar}\\right)\n",
    "H_n\\left(\\sqrt{\\frac{m\\omega}{\\hbar}}x\\right)$$\n",
    "\n",
    "Let's see whether they look like those. There are some special functions in the Numpy library, and some more in Scipy. Hermite Polynomials are in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.hermite import Hermite\n",
    "from math import factorial\n",
    "def ho_evec(x,n,m,ohm):\n",
    "    vec = [0]*9\n",
    "    vec[n] = 1\n",
    "    Hn = Hermite(vec)\n",
    "    return (1/np.sqrt(2**n*factorial(n)))*pow(m*ohm/np.pi,0.25)*np.exp(-0.5*m*ohm*x**2)*Hn(x*np.sqrt(m*ohm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the first function to our solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,ho_evec(x,0,1,1),label=\"Analytic\")\n",
    "plt.plot(x,-U[:,0]/np.sqrt(h),label=\"Numeric\")\n",
    "plt.xlabel('x (bohr)')\n",
    "plt.ylabel(r'$\\psi(x)$')\n",
    "plt.title(\"Comparison of numeric and analytic solutions to the Harmonic Oscillator\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agreement is almost exact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **subplot** command to put multiple comparisons in different panes on a single plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "phase_correction = [-1,1,1,-1,-1,1]\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.plot(x,ho_evec(x,i,1,1),label=\"Analytic\")\n",
    "    plt.plot(x,phase_correction[i]*U[:,i]/np.sqrt(h),label=\"Numeric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than phase errors (which I've corrected with a little hack: can you find it?), the agreement is pretty good, although it gets worse the higher in energy we get, in part because we used only 50 points.\n",
    "\n",
    "The Scipy module has many more special functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import airy,jn,eval_chebyt,eval_legendre\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(2,2,1)\n",
    "x = np.linspace(-1,1)\n",
    "Ai,Aip,Bi,Bip = airy(x)\n",
    "plt.plot(x,Ai)\n",
    "plt.plot(x,Aip)\n",
    "plt.plot(x,Bi)\n",
    "plt.plot(x,Bip)\n",
    "plt.title(\"Airy functions\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "x = np.linspace(0,10)\n",
    "for i in range(4):\n",
    "    plt.plot(x,jn(i,x))\n",
    "plt.title(\"Bessel functions\")\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "x = np.linspace(-1,1)\n",
    "for i in range(6):\n",
    "    plt.plot(x,eval_chebyt(i,x))\n",
    "plt.title(\"Chebyshev polynomials of the first kind\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "x = np.linspace(-1,1)\n",
    "for i in range(6):\n",
    "    plt.plot(x,eval_legendre(i,x))\n",
    "plt.title(\"Legendre polynomials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as Jacobi, Laguerre, Hermite polynomials, Hypergeometric functions, and many others. There's a full listing at the [Scipy Special Functions Page](http://docs.scipy.org/doc/scipy/reference/special.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares fitting\n",
    "Very often we deal with some data that we want to fit to some sort of expected behavior. Say we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raw_data = \"\"\"\\\n",
    "3.1905781584582433,0.028208609537968457\n",
    "4.346895074946466,0.007160804747670053\n",
    "5.374732334047101,0.0046962988461934805\n",
    "8.201284796573875,0.0004614473299618756\n",
    "10.899357601713055,0.00005038370219939726\n",
    "16.295503211991434,4.377451812785309e-7\n",
    "21.82012847965739,3.0799922117601088e-9\n",
    "32.48394004282656,1.524776208284536e-13\n",
    "43.53319057815846,5.5012073588707224e-18\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a section below on parsing CSV data. We'll steal the parser from that. For an explanation, skip ahead to that section. Otherwise, just assume that this is a way to parse that text into a numpy array that we can plot and do other analyses with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in raw_data.splitlines():\n",
    "    words = line.split(',')\n",
    "    data.append(list(map(float,words)))\n",
    "data = np.array(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Raw Data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.plot(data[:,0],data[:,1],'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we expect the data to have an exponential decay, we can plot it using a semi-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.title(\"Raw Data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.semilogy(data[:,0],data[:,1],'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pure exponential decay like this, we can fit the log of the data to a straight line. The above plot suggests this is a good approximation. Given a function\n",
    "$$ y = Ae^{-ax} $$\n",
    "$$ \\log(y) = \\log(A) - ax$$\n",
    "Thus, if we fit the log of the data versus x, we should get a straight line with slope $a$, and an intercept that gives the constant $A$.\n",
    "\n",
    "There's a numpy function called **polyfit** that will fit data to a polynomial form. We'll use this to fit to a straight line (a polynomial of order 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params = np.polyfit(data[:,0],np.log(data[:,1]),1)\n",
    "a = params[0]\n",
    "A = np.exp(params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether this curve fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1,45)\n",
    "plt.title(\"Raw Data\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.semilogy(data[:,0],data[:,1],'bo')\n",
    "plt.semilogy(x,A*np.exp(a*x),'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more complicated functions, we may not be able to get away with fitting to a simple polynomial. Consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gauss_data = \"\"\"\\\n",
    "-0.9902286902286903,1.4065274110372852e-19\n",
    "-0.7566104566104566,2.2504438576596563e-18\n",
    "-0.5117810117810118,1.9459459459459454\n",
    "-0.31887271887271884,10.621621621621626\n",
    "-0.250997150997151,15.891891891891893\n",
    "-0.1463309463309464,23.756756756756754\n",
    "-0.07267267267267263,28.135135135135133\n",
    "-0.04426734426734419,29.02702702702703\n",
    "-0.0015939015939017698,29.675675675675677\n",
    "0.04689304689304685,29.10810810810811\n",
    "0.0840994840994842,27.324324324324326\n",
    "0.1700546700546699,22.216216216216214\n",
    "0.370878570878571,7.540540540540545\n",
    "0.5338338338338338,1.621621621621618\n",
    "0.722014322014322,0.08108108108108068\n",
    "0.9926849926849926,-0.08108108108108646\"\"\"\n",
    "\n",
    "data = []\n",
    "for line in gauss_data.splitlines():\n",
    "    words = line.split(',')\n",
    "    data.append(list(map(float,words)))\n",
    "data = np.array(data)\n",
    "\n",
    "plt.plot(data[:,0],data[:,1],'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks more Gaussian than exponential. If we wanted to, we could use **polyfit** for this as well, but let's use the **curve_fit** function from Scipy, which can fit to arbitrary functions. You can learn more using help(curve_fit).\n",
    "\n",
    "First define a general Gaussian function to fit to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def gauss(x,A,a): return A*np.exp(a*x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit to it using **curve_fit**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "params,conv = scipy.optimize.curve_fit(gauss,data[:,0],data[:,1])\n",
    "x = np.linspace(-1,1)\n",
    "plt.plot(data[:,0],data[:,1],'bo')\n",
    "A,a = params\n",
    "plt.plot(x,gauss(x,A,a),'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **curve_fit** routine we just used is built on top of a very good general **minimization** capability in Scipy. You can learn more [at the scipy documentation pages](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo, random numbers, and computing $\\pi$\n",
    "Many methods in scientific computing rely on Monte Carlo integration, where a sequence of (pseudo) random numbers are used to approximate the integral of a function. Python has good random number generators in the standard library. The **random()** function gives pseudorandom numbers uniformly distributed between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rands = []\n",
    "for i in range(100):\n",
    "    rands.append(random.random())\n",
    "plt.plot(rands)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**random()** uses the [Mersenne Twister](http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html) algorithm, which is a highly regarded pseudorandom number generator. There are also functions to generate random integers, to randomly shuffle a list, and functions to pick random numbers from a particular distribution, like the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "grands = []\n",
    "for i in range(100):\n",
    "    grands.append(random.gauss(0,1))\n",
    "plt.plot(grands)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally more efficient to generate a list of random numbers all at once, particularly if you're drawing from a non-uniform distribution. Numpy has functions to generate vectors and matrices of particular types of random distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.random.rand(100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first programs I ever wrote was a program to compute $\\pi$ by taking random numbers as x and y coordinates, and counting how many of them were in the unit circle. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "npts = 5000\n",
    "xs = 2*np.random.rand(npts)-1\n",
    "ys = 2*np.random.rand(npts)-1\n",
    "r = xs**2+ys**2\n",
    "ninside = (r<1).sum()\n",
    "plt.figure(figsize=(6,6)) # make the figure square\n",
    "plt.title(\"Approximation to pi = %f\" % (4*ninside/float(npts)))\n",
    "plt.plot(xs[r<1],ys[r<1],'b.')\n",
    "plt.plot(xs[r>1],ys[r>1],'r.')\n",
    "plt.figure(figsize=(8,6)) # change the figsize back to 4x3 for the rest of the notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the program is that the ratio of the area of the unit circle to the square that inscribes it is $\\pi/4$, so by counting the fraction of the random points in the square that are inside the circle, we get increasingly good estimates to $\\pi$. \n",
    "\n",
    "The above code uses some higher level Numpy tricks to compute the radius of each point in a single line, to count how many radii are below one in a single line, and to filter the x,y points based on their radii. To be honest, I rarely write code like this: I find some of these Numpy tricks a little too cute to remember them, and I'm more likely to use a list comprehension (see below) to filter the points I want, since I can remember that.\n",
    "\n",
    "As methods of computing $\\pi$ go, this is among the worst. A much better method is to use Leibniz's expansion of arctan(1):\n",
    "\n",
    "$$\\frac{\\pi}{4} = \\sum_k \\frac{(-1)^k}{2*k+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "total = 0\n",
    "for k in range(n):\n",
    "    total += pow(-1,k)/(2*k+1.0)\n",
    "print(4*total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested a great method, check out [Ramanujan's method](http://en.wikipedia.org/wiki/Approximations_of_%CF%80). This converges so fast you really need arbitrary precision math to display enough decimal places. You can do this with the Python **decimal** module, if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Integration\n",
    "Integration can be hard, and sometimes it's easier to work out a definite integral using an approximation. For example, suppose we wanted to figure out the integral:\n",
    "\n",
    "$$\\int_0^\\infty\\exp(-x)dx=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def f(x): return np.exp(-x)\n",
    "x = np.linspace(0,10)\n",
    "plt.plot(x,np.exp(-x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy has a numerical integration routine **quad** (since sometimes numerical integration is called *quadrature*), that we can use for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "scipy.integrate.quad(f,0,np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also 2d and 3d numerical integrators in Scipy. [See the docs](http://docs.scipy.org/doc/scipy/reference/integrate.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Fourier Transform and Signal Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often we want to use FFT techniques to help obtain the signal from noisy data. Scipy has several different options for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft,fftfreq\n",
    "\n",
    "npts = 4000\n",
    "nplot = int(npts/10)\n",
    "t = np.linspace(0,120,npts)\n",
    "def acc(t): return 10*np.sin(2*np.pi*2.0*t) + 5*np.sin(2*np.pi*8.0*t) + 2*np.random.rand(npts)\n",
    "\n",
    "signal = acc(t)\n",
    "\n",
    "FFT = abs(scipy.fftpack.fft(signal))\n",
    "freqs = scipy.fftpack.fftfreq(npts, t[1]-t[0])\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(t[:nplot], signal[:nplot])\n",
    "plt.subplot(212)\n",
    "plt.plot(freqs,20*np.log10(FFT),',')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are additional signal processing routines in Scipy that you can [read about here](http://docs.scipy.org/doc/scipy/reference/tutorial/signal.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
